{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing configs/selfsup/relative_loc/relative-loc_resnet50_8xb64-steplr-70e_in1k_colab.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile configs/selfsup/relative_loc/relative-loc_resnet50_8xb64-steplr-70e_in1k_colab.py\n",
    "_base_ = [\n",
    "    '../_base_/models/relative-loc.py',\n",
    "    '../_base_/datasets/imagenet_relative-loc.py',\n",
    "    '../_base_/schedules/sgd_steplr-200e_in1k.py',\n",
    "    '../_base_/default_runtime.py',\n",
    "]\n",
    "\n",
    "default_hooks = dict(logger=dict(type='LoggerHook', interval=10))\n",
    "\n",
    "# optimizer wrapper\n",
    "optimizer = dict(type='SGD', lr=0.2, momentum=0.9, weight_decay=1e-4)\n",
    "optim_wrapper = dict(\n",
    "    type='OptimWrapper',\n",
    "    optimizer=optimizer,\n",
    "    paramwise_cfg=dict(custom_keys={\n",
    "        'neck': dict(decay_mult=5.0),\n",
    "        'head': dict(decay_mult=5.0)\n",
    "    }))\n",
    "\n",
    "# learning rate scheduler\n",
    "param_scheduler = [dict(type='MultiStepLR', by_epoch=True, milestones=[1, 2])]\n",
    "\n",
    "# runtime settings\n",
    "# pre-train for 70 epochs\n",
    "train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=70)\n",
    "# the max_keep_ckpts controls the max number of ckpt file in your work_dirs\n",
    "# if it is 3, when CheckpointHook (in mmcv) saves the 4th ckpt\n",
    "# it will remove the oldest one to keep the number of total ckpts as 3\n",
    "default_hooks = dict(\n",
    "    checkpoint=dict(type='CheckpointHook', interval=1, max_keep_ckpts=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the basic config file\n",
    "from mmengine.config import Config\n",
    "cfg = Config.fromfile('configs/selfsup/relative_loc/relative-loc_resnet50_8xb64-steplr-70e_in1k_colab.py')\n",
    "\n",
    "# Specify the data settings\n",
    "cfg.train_dataloader.batch_size = 8\n",
    "cfg.train_dataloader.num_workers = 2\n",
    "\n",
    "# Specify the optimizer\n",
    "cfg.optimizer = dict(type='SGD', lr=0.005, momentum=0.9, weight_decay=0.0001)\n",
    "cfg.optim_wrapper.clip_grad = None\n",
    "\n",
    "# Specify the learning rate scheduler\n",
    "cfg.param_scheduler = [dict(type='MultiStepLR', by_epoch=True, milestones=[1, 2])]\n",
    "\n",
    "# Modify runtime setting\n",
    "cfg.train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=2)\n",
    "\n",
    "# Specify the work directory\n",
    "cfg.work_dir = './work_dirs/selfsup/relative-loc_resnet50_8xb64-steplr-70e_in1k_colab'\n",
    "\n",
    "# Output logs for every 10 iterations\n",
    "cfg.default_hooks.logger.interval = 10\n",
    "# Set the random seed and enable the deterministic option of cuDNN\n",
    "# to keep the results' reproducible.\n",
    "cfg.randomness = dict(seed=0, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/28 21:02:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.8.17 (default, Jul  5 2023, 21:04:15) [GCC 11.2.0]\n",
      "    CUDA available: False\n",
      "    numpy_random_seed: 0\n",
      "    GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "    PyTorch: 2.0.1\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201703\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=0, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.15.2\n",
      "    OpenCV: 4.8.0\n",
      "    MMEngine: 0.8.4\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: False\n",
      "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 0\n",
      "    deterministic: True\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "08/28 21:02:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
      "data_root = 'data/imagenet/'\n",
      "dataset_type = 'mmcls.ImageNet'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(interval=1, max_keep_ckpts=3, type='CheckpointHook'),\n",
      "    logger=dict(interval=10, type='LoggerHook'),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    runtime_info=dict(type='RuntimeInfoHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    timer=dict(type='IterTimerHook'))\n",
      "default_scope = 'mmselfsup'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=False,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "load_from = None\n",
      "log_level = 'INFO'\n",
      "log_processor = dict(\n",
      "    custom_cfg=[\n",
      "        dict(data_src='', method='mean', window_size='global'),\n",
      "    ],\n",
      "    window_size=10)\n",
      "model = dict(\n",
      "    backbone=dict(\n",
      "        depth=50,\n",
      "        in_channels=3,\n",
      "        norm_cfg=dict(type='BN'),\n",
      "        out_indices=[\n",
      "            4,\n",
      "        ],\n",
      "        type='ResNet'),\n",
      "    data_preprocessor=dict(\n",
      "        bgr_to_rgb=True,\n",
      "        mean=[\n",
      "            124,\n",
      "            117,\n",
      "            104,\n",
      "        ],\n",
      "        std=[\n",
      "            59,\n",
      "            58,\n",
      "            58,\n",
      "        ],\n",
      "        type='mmselfsup.RelativeLocDataPreprocessor'),\n",
      "    head=dict(\n",
      "        in_channels=4096,\n",
      "        init_cfg=[\n",
      "            dict(layer='Linear', std=0.005, type='Normal'),\n",
      "            dict(layer=[\n",
      "                '_BatchNorm',\n",
      "                'GroupNorm',\n",
      "            ], type='Constant', val=1),\n",
      "        ],\n",
      "        loss=dict(type='mmcls.CrossEntropyLoss'),\n",
      "        num_classes=8,\n",
      "        type='ClsHead',\n",
      "        with_avg_pool=False),\n",
      "    neck=dict(\n",
      "        in_channels=2048,\n",
      "        out_channels=4096,\n",
      "        type='RelativeLocNeck',\n",
      "        with_avg_pool=True),\n",
      "    type='RelativeLoc')\n",
      "optim_wrapper = dict(\n",
      "    clip_grad=None,\n",
      "    optimizer=dict(lr=0.2, momentum=0.9, type='SGD', weight_decay=0.0001),\n",
      "    paramwise_cfg=dict(\n",
      "        custom_keys=dict(head=dict(decay_mult=5.0), neck=dict(\n",
      "            decay_mult=5.0))),\n",
      "    type='OptimWrapper')\n",
      "optimizer = dict(lr=0.005, momentum=0.9, type='SGD', weight_decay=0.0001)\n",
      "param_scheduler = [\n",
      "    dict(by_epoch=True, milestones=[\n",
      "        1,\n",
      "        2,\n",
      "    ], type='MultiStepLR'),\n",
      "]\n",
      "randomness = dict(deterministic=True, seed=0)\n",
      "resume = False\n",
      "train_cfg = dict(max_epochs=2, type='EpochBasedTrainLoop')\n",
      "train_dataloader = dict(\n",
      "    batch_size=8,\n",
      "    collate_fn=dict(type='default_collate'),\n",
      "    dataset=dict(\n",
      "        ann_file='meta/train.txt',\n",
      "        data_prefix=dict(img_path='train/'),\n",
      "        data_root='data/imagenet/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(scale=292, type='Resize'),\n",
      "            dict(size=255, type='RandomCrop'),\n",
      "            dict(keep_channels=True, prob=0.66, type='RandomGrayscale'),\n",
      "            dict(type='RandomPatchWithLabels'),\n",
      "            dict(\n",
      "                meta_keys=[\n",
      "                    'img_path',\n",
      "                ],\n",
      "                pseudo_label_keys=[\n",
      "                    'patch_box',\n",
      "                    'patch_label',\n",
      "                    'unpatched_img',\n",
      "                ],\n",
      "                type='PackSelfSupInputs'),\n",
      "        ],\n",
      "        type='mmcls.ImageNet'),\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(scale=292, type='Resize'),\n",
      "    dict(size=255, type='RandomCrop'),\n",
      "    dict(keep_channels=True, prob=0.66, type='RandomGrayscale'),\n",
      "    dict(type='RandomPatchWithLabels'),\n",
      "    dict(\n",
      "        meta_keys=[\n",
      "            'img_path',\n",
      "        ],\n",
      "        pseudo_label_keys=[\n",
      "            'patch_box',\n",
      "            'patch_label',\n",
      "            'unpatched_img',\n",
      "        ],\n",
      "        type='PackSelfSupInputs'),\n",
      "]\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='SelfSupVisualizer',\n",
      "    vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "    ])\n",
      "work_dir = './work_dirs/selfsup/relative-loc_resnet50_8xb64-steplr-70e_in1k_colab'\n",
      "\n",
      "08/28 21:02:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "08/28 21:02:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "08/28 21:02:07 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::add_ encountered 69 time(s)\n",
      "08/28 21:02:07 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
      "08/28 21:02:07 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "data_preprocessor, head, head.fc_cls, head.loss, neck, neck.avgpool, neck.bn, neck.dropout, neck.fc, neck.relu\n",
      "08/28 21:02:07 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::batch_norm encountered 53 time(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+--------------------------+----------------------+------------+--------------+\n",
      "|\u001b[1m \u001b[0m\u001b[1mmodule                  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m#parameters or shape\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m#flops    \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m#activations\u001b[0m\u001b[1m \u001b[0m|\n",
      "+--------------------------+----------------------+------------+--------------+\n",
      "| model                    | 40.33M               | 89.51M     | 0.155M       |\n",
      "|  backbone                |  23.508M             |  89.51M    |  0.155M      |\n",
      "|   backbone.conv1         |   9.408K             |   0.706M   |   4.8K       |\n",
      "|    backbone.conv1.weight |    (64, 3, 7, 7)     |            |              |\n",
      "|   backbone.bn1           |   0.128K             |   24K      |   0          |\n",
      "|    backbone.bn1.weight   |    (64,)             |            |              |\n",
      "|    backbone.bn1.bias     |    (64,)             |            |              |\n",
      "|   backbone.layer1        |   0.216M             |   5.941M   |   38.016K    |\n",
      "|    backbone.layer1.0     |    75.008K           |    2.077M  |    17.28K    |\n",
      "|    backbone.layer1.1     |    70.4K             |    1.932M  |    10.368K   |\n",
      "|    backbone.layer1.2     |    70.4K             |    1.932M  |    10.368K   |\n",
      "|   backbone.layer2        |   1.22M              |   15.265M  |   44.928K    |\n",
      "|    backbone.layer2.0     |    0.379M            |    5.1M    |    17.28K    |\n",
      "|    backbone.layer2.1     |    0.28M             |    3.388M  |    9.216K    |\n",
      "|    backbone.layer2.2     |    0.28M             |    3.388M  |    9.216K    |\n",
      "|    backbone.layer2.3     |    0.28M             |    3.388M  |    9.216K    |\n",
      "|   backbone.layer3        |   7.098M             |   22.578M  |   33.024K    |\n",
      "|    backbone.layer3.0     |    1.512M            |    5.752M  |    9.984K    |\n",
      "|    backbone.layer3.1     |    1.117M            |    3.365M  |    4.608K    |\n",
      "|    backbone.layer3.2     |    1.117M            |    3.365M  |    4.608K    |\n",
      "|    backbone.layer3.3     |    1.117M            |    3.365M  |    4.608K    |\n",
      "|    backbone.layer3.4     |    1.117M            |    3.365M  |    4.608K    |\n",
      "|    backbone.layer3.5     |    1.117M            |    3.365M  |    4.608K    |\n",
      "|   backbone.layer4        |   14.965M            |   44.996M  |   33.792K    |\n",
      "|    backbone.layer4.0     |    6.04M             |    18.165M |    15.36K    |\n",
      "|    backbone.layer4.1     |    4.463M            |    13.415M |    9.216K    |\n",
      "|    backbone.layer4.2     |    4.463M            |    13.415M |    9.216K    |\n",
      "|  neck                    |  16.79M              |            |              |\n",
      "|   neck.fc                |   16.781M            |            |              |\n",
      "|    neck.fc.weight        |    (4096, 4096)      |            |              |\n",
      "|    neck.fc.bias          |    (4096,)           |            |              |\n",
      "|   neck.bn                |   8.192K             |            |              |\n",
      "|    neck.bn.weight        |    (4096,)           |            |              |\n",
      "|    neck.bn.bias          |    (4096,)           |            |              |\n",
      "|  head.fc_cls             |  32.776K             |            |              |\n",
      "|   head.fc_cls.weight     |   (8, 4096)          |            |              |\n",
      "|   head.fc_cls.bias       |   (8,)               |            |              |\n",
      "+--------------------------+----------------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmengine.runner import Runner\n",
    "from mmengine.analysis import get_model_complexity_info\n",
    "# build the runner from config\n",
    "runner = Runner.from_cfg(cfg)\n",
    "input_shape = (3, 3, 10, 10)\n",
    "analysis_results = get_model_complexity_info(runner.model, input_shape)\n",
    "print(analysis_results['out_table'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/28 21:03:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- neck.fc.weight:lr=0.2\n",
      "08/28 21:03:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- neck.fc.weight:weight_decay=0.0005\n",
      "08/28 21:03:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- neck.fc.weight:decay_mult=5.0\n",
      "08/28 21:03:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- neck.fc.bias:lr=0.2\n",
      "08/28 21:03:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- neck.fc.bias:weight_decay=0.0005\n",
      "08/28 21:03:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- neck.fc.bias:decay_mult=5.0\n",
      "08/28 21:03:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- neck.bn.weight:lr=0.2\n",
      "08/28 21:03:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- neck.bn.weight:weight_decay=0.0005\n",
      "08/28 21:03:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- neck.bn.weight:decay_mult=5.0\n",
      "08/28 21:03:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- neck.bn.bias:lr=0.2\n",
      "08/28 21:03:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- neck.bn.bias:weight_decay=0.0005\n",
      "08/28 21:03:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- neck.bn.bias:decay_mult=5.0\n",
      "08/28 21:03:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- head.fc_cls.weight:lr=0.2\n",
      "08/28 21:03:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- head.fc_cls.weight:weight_decay=0.0005\n",
      "08/28 21:03:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- head.fc_cls.weight:decay_mult=5.0\n",
      "08/28 21:03:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- head.fc_cls.bias:lr=0.2\n",
      "08/28 21:03:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- head.fc_cls.bias:weight_decay=0.0005\n",
      "08/28 21:03:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- head.fc_cls.bias:decay_mult=5.0\n",
      "08/28 21:03:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "08/28 21:03:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "08/28 21:03:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /home/max/Documents/mmselfsup/mmselfsup/work_dirs/selfsup/relative-loc_resnet50_8xb64-steplr-70e_in1k_colab.\n",
      "08/28 21:04:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 10/163]  base_lr: 2.0000e-01 lr: 2.0000e-01  eta: 0:43:05  time: 8.1833  data_time: 0.0440  loss: 23.5399\n",
      "08/28 21:05:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 20/163]  base_lr: 2.0000e-01 lr: 2.0000e-01  eta: 0:39:47  time: 7.4226  data_time: 0.0213  loss: 22.3041\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m runner\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmengine/runner/runner.py:1745\u001b[0m, in \u001b[0;36mRunner.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1741\u001b[0m \u001b[39m# Maybe compile the model according to options in self.cfg.compile\u001b[39;00m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# This must be called **AFTER** model has been wrapped.\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_compile(\u001b[39m'\u001b[39m\u001b[39mtrain_step\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1745\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_loop\u001b[39m.\u001b[39;49mrun()  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_hook(\u001b[39m'\u001b[39m\u001b[39mafter_run\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1747\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmengine/runner/loops.py:96\u001b[0m, in \u001b[0;36mEpochBasedTrainLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunner\u001b[39m.\u001b[39mcall_hook(\u001b[39m'\u001b[39m\u001b[39mbefore_train\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epoch \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_epochs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_epoch()\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decide_current_val_interval()\n\u001b[1;32m     99\u001b[0m     \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunner\u001b[39m.\u001b[39mval_loop \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    100\u001b[0m             \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epoch \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_begin\n\u001b[1;32m    101\u001b[0m             \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epoch \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmengine/runner/loops.py:112\u001b[0m, in \u001b[0;36mEpochBasedTrainLoop.run_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunner\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m    111\u001b[0m \u001b[39mfor\u001b[39;00m idx, data_batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader):\n\u001b[0;32m--> 112\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_iter(idx, data_batch)\n\u001b[1;32m    114\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunner\u001b[39m.\u001b[39mcall_hook(\u001b[39m'\u001b[39m\u001b[39mafter_train_epoch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmengine/runner/loops.py:128\u001b[0m, in \u001b[0;36mEpochBasedTrainLoop.run_iter\u001b[0;34m(self, idx, data_batch)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunner\u001b[39m.\u001b[39mcall_hook(\n\u001b[1;32m    124\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mbefore_train_iter\u001b[39m\u001b[39m'\u001b[39m, batch_idx\u001b[39m=\u001b[39midx, data_batch\u001b[39m=\u001b[39mdata_batch)\n\u001b[1;32m    125\u001b[0m \u001b[39m# Enable gradient accumulation mode and avoid unnecessary gradient\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[39m# synchronization during gradient accumulation process.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39m# outputs should be a dict of loss.\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunner\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtrain_step(\n\u001b[1;32m    129\u001b[0m     data_batch, optim_wrapper\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunner\u001b[39m.\u001b[39;49moptim_wrapper)\n\u001b[1;32m    131\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunner\u001b[39m.\u001b[39mcall_hook(\n\u001b[1;32m    132\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mafter_train_iter\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    133\u001b[0m     batch_idx\u001b[39m=\u001b[39midx,\n\u001b[1;32m    134\u001b[0m     data_batch\u001b[39m=\u001b[39mdata_batch,\n\u001b[1;32m    135\u001b[0m     outputs\u001b[39m=\u001b[39moutputs)\n\u001b[1;32m    136\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmengine/model/base_model/base_model.py:116\u001b[0m, in \u001b[0;36mBaseModel.train_step\u001b[0;34m(self, data, optim_wrapper)\u001b[0m\n\u001b[1;32m    114\u001b[0m     losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_forward(data, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    115\u001b[0m parsed_losses, log_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_losses(losses)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m optim_wrapper\u001b[39m.\u001b[39;49mupdate_params(parsed_losses)\n\u001b[1;32m    117\u001b[0m \u001b[39mreturn\u001b[39;00m log_vars\n",
      "File \u001b[0;32m~/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmengine/optim/optimizer/optimizer_wrapper.py:200\u001b[0m, in \u001b[0;36mOptimWrapper.update_params\u001b[0;34m(self, loss, step_kwargs, zero_kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     zero_kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m    199\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_loss(loss)\n\u001b[0;32m--> 200\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[1;32m    201\u001b[0m \u001b[39m# Update parameters only if `self._inner_count` is divisible by\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39m# `self._accumulative_counts` or `self._inner_count` equals to\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39m# `self._max_counts`\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshould_update():\n",
      "File \u001b[0;32m~/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmengine/optim/optimizer/optimizer_wrapper.py:224\u001b[0m, in \u001b[0;36mOptimWrapper.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m, loss: torch\u001b[39m.\u001b[39mTensor, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform gradient back propagation.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \n\u001b[1;32m    211\u001b[0m \u001b[39m    Provide unified ``backward`` interface compatible with automatic mixed\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m        kwargs: Keyword arguments passed to :meth:`torch.Tensor.backward`.\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    225\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/openmmlab/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/openmmlab/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
